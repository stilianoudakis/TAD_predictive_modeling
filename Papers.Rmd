# Notes on TAD prediction and enrichment

## TAD predictions

- Xiong, Kyle, and Jian Ma. “Revealing Hi-C Subcompartments by Imputing High-Resolution Inter-Chromosomal Chromatin Interactions.” BioRxiv, January 1, 2018, 505503. https://doi.org/10.1101/505503. - SNIPER - 3D subcompartment (A1, A2, B1, B2, B3) identification from low-coverage Hi-C datasets. A neural network based on a denoising autoencoder (9 layers) and multi-layer perceptron. Sigmoidal activation of inputs, ReLU, softmax on outputs. Dropout, binary cross-entropy. exp(-1/C) transformation of Hi-C matrices. Applied to Gm12878 and 8 additional cell types to compare subcompartment changes. Compared with Rao2014 annotations, outperforms Gaussian HMM and MEGABASE. https://github.com/ma-compbio/SNIPER
- Di Pierro, Michele, Ryan R. Cheng, Erez Lieberman Aiden, Peter G. Wolynes, and José N. Onuchic. “De Novo Prediction of Human Chromosome Structures: Epigenetic Marking Patterns Encode Genome Architecture.” Proceedings of the National Academy of Sciences of the United States of America 114, no. 46 (14 2017): 12126–31. https://doi.org/10.1073/pnas.1714980114. - MEGABASE - prediction of A1, A2, B1, B2, B3, B4 subcompartments from epigenomic features using deep learning. 84+11 histone- and ChIP-seq. Gm12878. Then, modeling 3D structure.

- Schreiber, Jacob, Timothy J Durham, Jeffrey Bilmes, and William Stafford Noble. “Multi-Scale Deep Tensor Factorization Learns a Latent Representation of the Human Epigenome,” July 8, 2018. https://doi.org/10.1101/364976. - Avocado - cell type-specific epigenome representation using a deep neural network tensor factorization method. Predict gene expression, promoter-enhancer interactions, TAD boundaries, frequently interacting regions (FIREs), missing cell-specific epigenomic marks. Comparison with ChromImpute, PREDICTD. Improves PREDICTD by inserting a deep neural network to nonlinearly combine decomposed latent factors. Model, imputed marks can be downloaded at https://noble.gs.washington.edu/proj/avocado/. Data source: TAD boundaries and FIRE scores are from supplementary of A. D. Schmitt, M. Hu, I. Jung, Z. Xu, Y. Qiu, C. L. Tan, Y. Li, S. Lin, Y. Lin, C. L. Barr, and B. Ren. A compendium of chromatin contact maps reveals spatially active regions in the human genome. Cell Reports, 17:2042–2059, 2016. Promoter-enhancer interactions are from https://github.com/shwhalen/targetfinder/tree/master/paper/targetfinder/combined/output-epw

- Hong, Seungpyo, and Dongsup Kim. “Computational Characterization of Chromatin Domain Boundary-Associated Genomic Elements.” Nucleic Acids Research 45, no. 18 (October 13, 2017): 10403–14. https://doi.org/10.1093/nar/gkx738. - Feature selection for predicting TAD boundaries. In addition to CTCF, ZNF143, YY1, DNAse, H3K36me3, TSSs, RNA Pol II, SP1, ZNF274, SIX5. Linear model, similar to forward selection. Data from UCSC, in supplementary. Distinguishing boundaries and centers. Distinguishing cell type-specific and clustered TFBSs.

- Mourad, Raphaël, and Olivier Cuvier. “Computational Identification of Genomic Features That Influence 3D Chromatin Domain Formation.” Edited by Kai Tan. PLOS Computational Biology 12, no. 5 (May 20, 2016): e1004908. https://doi.org/10.1371/journal.pcbi.1004908. - Predicting TAD boundaries from transcription factor binding sites (TFBSs). Multiple logistic regression with lasso L1 penalization, allows for interactions, positive/negative effect. Outperforms enrichment test, random forest (https://github.com/aloysius-lim/bigrf). For humans, CTCF, Cohesin, ZNF134, Polycomb group proteins are positive predictors, P300, RXRA, BCL11A and ELK1 are negative predictors. Tested for the effect of SNPs in CTCF binding sites (SNPs destroy TAD boundaries), the effect of slightly imprecise boundaries. Hi-C bins are assigned 1/0 for overlapping/nonoverlapping TFBSs, or % overlap if it is partial. No mentioning class disbalance problem, no cross-validation, no explicit distance consideration, no histone marks. HiCfeat R package, https://sites.google.com/site/raphaelmouradeng/home/programs

- Sefer, Emre, and Carl Kingsford. “Semi-Nonparametric Modeling of Topological Domain Formation from Epigenetic Data.” In Algorithms in Bioinformatics, edited by Mihai Pop and Hélène Touzet, 9289:148–61. Berlin, Heidelberg: Springer Berlin Heidelberg, 2015. https://doi.org/10.1007/978-3-662-48221-6_11. - Non-parametric approach, called nTDP, based on Bernstein polynomials for modeling TADs from histone modifications. Intro about histone modifications enriched in TAD boundaries (H3K4me3 and H3K27ac). Math, statistics, assumptions, regularization. Parameters of the model should be estimated via training, Cross-validation to prevent overfitting. Hi-C data from Human IMR90, ES, mouse ES. Binning ChIP-seq and DNAse-seq data at 40kb resolution as log RPKMs, consensus TAD boundaries from Armatus. The sets of 4 and 6 modifications that were most informative are: {H3K36me3, H3K4me1, H3K4me3, H3K9me3} and {H3K4me3, H3K79me2, H3K27ac, H3K9me3, H3K36me3, H4K20me1}. chromatin states from Segway are not important. https://www.cs.cmu.edu/~ckingsf/research/ntdp/

- Bednarz, Paweł, and Bartek Wilczyński. “Supervised Learning Method for Predicting Chromatin Boundary Associated Insulator Elements.” Journal of Bioinformatics and Computational Biology 12, no. 06 (December 2014): 1442006. https://doi.org/10.1142/S0219720014420062. - Predicting TAD boundaries using training data, and making new predictions. Bayesian network (BNFinder method), random forest vs. basic k-means clustering, ChromHMM, cdBEST. Using sequence k-mers and ChIP-seq data from modENCODE for prediction - CTCF ChIP-seq performs best. Used Boruta package for feature selection. Bayesian network performs best. To read on their BNFinder method

- Huang, Jialiang, Eugenio Marco, Luca Pinello, and Guo-Cheng Yuan. “Predicting Chromatin Organization Using Histone Marks.” Genome Biology 16, no. 1 (December 2015). https://doi.org/10.1186/s13059-015-0740-z. - Predicting chromatin interaction hubs and TAD boundaries from Hi-C data and 9 histone marks using BART - Bayesian Additive Regression Trees. Rank chromatin interactions into high ("hubs"), median, low, non-significant. Hubs are enriched in GWAS SNPs. Hubs are enriched in and predicted by H3K4me1, H3K27ac (markers of enhancers) in the center. Conservation, GC content, distance  to TSS did NOT improve predictions. The model trained using one cell type data predicts hubs in another. TAD boundaries from IMR90 (Dixon) vs. non-boundary genomic loci with similar interaction frequency. CTCF, then negative H3K4me1, are the most predictive, but H3K4me3 is the second most enriched.

- Zhou, Jian, and Olga G. Troyanskaya. “Global Quantitative Modeling of Chromatin Factor Interactions.” Edited by Andrey Rzhetsky. PLoS Computational Biology 10, no. 3 (March 27, 2014): e1003525. https://doi.org/10.1371/journal.pcbi.1003525. - A novel maximum entropy-based modeling approach to quantitatively capture interactions between chromatin factors at the same genomic location. Choosing the most uniform/least structured, maximum entropy. Pairwise and triplet interactions. L1 regularization.

- Bednarz, Paweł, and Bartek Wilczyński. “Supervised Learning Method for Predicting Chromatin Boundary Associated Insulator Elements.” Journal of Bioinformatics and Computational Biology 12, no. 6 (December 2014): 1442006. https://doi.org/10.1142/S0219720014420062. - Prediction of TAD boundaries in Drosophila using modENCODE data (>30 ChIP-Chip and ChIP-seq tracks). Starting from k-means, then Hidden Markov Models, then use cdBEST and BMFinder previously published methods. ROC curves. Boruta package for feature prioritization. Prediction of new boundaries.


## TAD epigenomics

- Yan, Koon-Kiu, Shaoke Lou, and Mark Gerstein. “MrTADFinder: A Network Modularity Based Approach to Identify Topologically Associating Domains in Multiple Resolutions.” PLoS Computational Biology 13, no. 7 (July 2017): e1005647. https://doi.org/10.1371/journal.pcbi.1005647. - MrTADFinder - network modularity-based TAD finder, community detection. Adapted for multiple resolutions with parameter y. Distance-dependent model. Optimization framework, Louvain algorithm to optimize Q. Confirmed Dixon's observations of H3K4me3, H3K27ac enrichment at TAD boundaries, depletion of H3K9me3, H3K27me3 enriched across resolutions. Genes, especially, housekeeping, are enriched near TAD boundaries. High- and extreme occupancy target regions (TF binding) are also enriched. CTCF enrichment is general. Logistic regression to predict TAD boundaries from TFBSs, prioritize which TFs are most important, Refs 18 and 19 refer to similar work. https://github.com/gersteinlab/MrTADFinder

- Krietenstein, Nils, Sameer Abraham, Sergey Venev, Nezar Abdennur, Johan Gibcus, Tsung-Han Hsieh, Krishna Mohan Parsi, et al. “Ultrastructural Details of Mammalian Chromosome Architecture.” Preprint. Genomics, May 17, 2019. https://doi.org/10.1101/639922. - Micro-C (MNase digestion Hi-C) technology and basic analysis. human embryonic stem cells H1-ESC and differentiated human foreskin fibroblasts (HFFc6). Captures standard Hi-C features, with many additional interaction peaks ("dots"). Enrichment of classical marks of TAD boundaries (Fig 3C) - RAD21, TAF1, PHF8, CTCF, TBP, POL2RA, YY1, and more.


## Epigenomic predictions

Many other in my `Zotero/Genome/Sequencing/Epigenomics/machine learning` collection

- Jaroszewicz, Artur, and Jason Ernst. “An Integrative Approach for Fine-Mapping Chromatin Interactions.” Preprint. Bioinformatics, April 11, 2019. https://doi.org/10.1101/605576. - X-SCNN - prediction of significant Hi-C interactions at highly improved resolution using TFBSs, histone marks, DNAse data (WIG format). A Siamese Convolutional Neural Network (SCNN) - two subnetworks with shared parameters predicting true interactions. HiCCUPS calls as true interactions, the same number of no interactions (balanced dataset). Keras with TensorFlow backend.https://github.com/ernstlab/X-SCNN. Describe this paper and the references from introduction about the use of epigenomic data for prediction.

- Zhang, Shilu, Deborah Chasman, Sara Knaack, and Sushmita Roy. “Prediction of High-Resolution Hi-C Interaction Matrices.” BioRxiv, September 1, 2018. https://doi.org/10.1101/406322. - HiC-Reg - predict contact counts from epigenomic reatures, across cell lines. Previous methods use binary classification framework. Random forest regression-based framework, counts for pairs of regions as outputs of a regression model from input one-dimensional regulatory signals. Three types of feature encoding, all include distance between pairs of regions. AUC to measure performance. Five-fold cross-validation. Out Of Bag variable importance for feature selection. Distance, together with CTCF and other features are the best predictive features. TAD detection using directionality index, comparison with Jaccard. https://github.com/Roy-lab/HiC-Reg

- Belokopytova, Polina, Evgeniy Mozheiko, Miroslav Nuriddinov, Daniil Fishman, and Veniamin Fishman. “Quantitative Prediction of Enhancer-Promoter Interactions.” BioRxiv, February 5, 2019. https://doi.org/10.1101/541011. - 3DPredictor - Prediction of enhnancer-promoter interactions from gene expression and CTCF binding. Also predict changes in 3D genome organization - genomic rearrangements. Previous prediction techniques. Benchmarking of TargetFinder, its performance is overestimated. Class balance slightly improves performance. Two random chromosomes for validation, the rest - for training. DISTANCE as predictor. Gradient boosting for prediction, significantly better than Random Forest. Other tools - EP2Vec, CTCF-MP, HiC-Reg. https://github.com/labdevgen/3DPredictor

- Singh, Shashank, Yang Yang, Barnabas Poczos, and Jian Ma. “Predicting Enhancer-Promoter Interaction from Genomic Sequence with Deep Neural Networks,” February 5, 2018. https://doi.org/10.1101/085241. - SPEID - predict enhancer-promoter interactions from sequence only using deep learning. Extends author's PEP work, compared with TargetFinder, not RIPPLE. Three layers, convolutional, recurrent, dense. AUROC, F1. Class imbalance is addressed by "data augmentation", similar to oversampling. Ro account for proximal features, enhancers are extended. Feature importance in different cell lines. https://github.com/ma-compbio/SPEID

- Yang, Yang, Ruochi Zhang, Shashank Singh, and Jian Ma. “Exploiting Sequence-Based Features for Predicting Enhancer-Promoter Interactions.” Bioinformatics (Oxford, England) 33, no. 14 (July 15, 2017): i252–60. https://doi.org/10.1093/bioinformatics/btx257. - Predicting enhancer-promoter interactions based on sequence only. Boosted tree ensemble model. Two ways: PEP-Motif (prediction from motifs detected in the sequences), and PEP-Word (word embedding model, achieved better performance). Compared with RIPPLE and TargetFinder. https://github.com/ma-compbio/PEP

- Cao, Qin, Christine Anyansi, Xihao Hu, Liangliang Xu, Lei Xiong, Wenshu Tang, Myth T. S. Mok, et al. “Reconstruction of Enhancer-Target Networks in 935 Samples of Human Primary Cells, Tissues and Cell Lines.” Nature Genetics 49, no. 10 (October 2017): 1428–36. https://doi.org/10.1038/ng.3950. - Enhancer-promoter prediction from cell-specific gene expression, methylation in windows around TSS, Roadmap, FANTOM5 CAGE, ENCODE DNAse, LASSO, Elastic Net, cross-validation. 935 cell lines, networks downloadable as cell-specific hg19 genomic coordinates-EnsemblIDs-confidence score (higher the better) (http://yiplab.cse.cuhk.edu.hk/jeme/). Activity of _multiple_ enhancers explains gene expression better. JEME method outperforms TargetFinder, PreSTIGE, Ripple, IM-PET, Ernst. Activity is confined within TADs. Depletion of CTCF between enhander-TSS pair. Closest enhancer is poor predictor of gene regulation, distance is the second best predictor, combined model is the best. JEME code https://github.com/yiplabcuhk/JEME

- Firpi, Hiram A., Duygu Ucar, and Kai Tan. “Discover Regulatory DNA Elements Using Chromatin Signatures and Artificial Neural Network.” Bioinformatics (Oxford, England) 26, no. 13 (July 1, 2010): 1579–86. https://doi.org/10.1093/bioinformatics/btq248.  -Predicting enhancers from histone marks using a time-delay neural network. Review of other prediction approaches. Genomic annotations are converted into two functions - mean and energy, used for predictions. Fisher discriminant analysis as a feature extraction/reduction technique (H3K4me1 is the most predictive, then H3K4me2, H3K4me3)

- He, Bing, Changya Chen, Li Teng, and Kai Tan. “Global View of Enhancer-Promoter Interactome in Human Cells.” Proceedings of the National Academy of Sciences of the United States of America 111, no. 21 (May 27, 2014): E2191-2199. https://doi.org/10.1073/pnas.1320308111. - IM-PET method. Enhancer-promoter interaction prediction from several features: enhancer-promoter activity profile correlation, TF-promoter correlation, enhancer-promoter coevolution, distance between enhancer and promoter. DISTANCE is the most important predictive feature, but helped by others. Random forest, SVM and logistic regression perform worse. Compared with four other methods (nearest promoter, PreSTIGE, Ernst, Thurman). Cross-validation, no overfitting. Enhancer-promoter interactions are more cell-type-specific than just enhancers. 

- Schuster-Böckler, Benjamin, and Ben Lehner. “Chromatin Organization Is a Major Influence on Regional Mutation Rates in Human Cancer Cells.” Nature 488, no. 7412 (August 23, 2012): 504–7. https://doi.org/10.1038/nature11273. - Cancer SNP density correlates with H3K9me3. 46 features, including Hi-C PCs, were tested for pairwise Pearson correlations.

## Prediction, general

- Twitter tread on class imbalance, https://twitter.com/goodfellow_ian/status/1127994332416364549

- Wei, Qiong, and Roland L. Dunbrack. “The Role of Balanced Training and Testing Data Sets for Binary Classifiers in Bioinformatics.” PloS One 8, no. 7 (2013): e67863. https://doi.org/10.1371/journal.pone.0067863. - Class imbalance. Intro into the problem, methods for addressing class imbalance. Balanced _training_ set is crucial for the best classifier performance, balanced _test_set also helps to improve the results. AUROC is not sensitive to class imbalance, MCC is. Illustrated on predicting missense mutations.

- Wallace, Byron C., Kevin Small, Carla E. Brodley, and Thomas A. Trikalinos. “Class Imbalance, Redux.” In 2011 IEEE 11th International Conference on Data Mining, 754–63. Vancouver, BC, Canada: IEEE, 2011. https://doi.org/10.1109/ICDM.2011.33. - Class imbalance is best addressed by bagging an ensemble of classifiers over balanced bootstrap training samples. Undersampling performs well, SMOTE and weighted SVM fail at high dimensions.

- Haibo He, and E.A. Garcia. “Learning from Imbalanced Data.” IEEE Transactions on Knowledge and Data Engineering 21, no. 9 (September 2009): 1263–84. https://doi.org/10.1109/TKDE.2008.239. - Class imbalance comprehensive review. Approaches (random under-/oversampling, SMOTE, others). Assessment metrics for imbalanced class learning. Disadvantages of ROC curves and advantages of PR curves (J. Davis, M. Goadrich, "The Relationship between Precision-Recall and ROC Curves", Proc. Int’l Conf. Machine Learning). 

- When the data is unbalanced, standard machine learning algorithms tend to be overwhelmed by the majority class, http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.711.8214

- C. Drummond, R. Holte, et al. C4. 5, class imbalance, and cost sensitivity: why under-sampling beats over-sampling. In Workshop on Learning from Imbalanced Datasets II. Citeseer, 2003

- Dal Pozzolo, Andrea, Olivier Caelen, Serge Waterschoot, and Gianluca Bontempi. “Racing for Unbalanced Methods Selection.” In International Conference on Intelligent Data Engineering and Automated Learning, 24–31. Springer, 2013. - F-racing strategy to select best performing method to deal with class imbalance. Overview of class imbalance techniques, including SMOTE, Ensemble methods. The F-Racing approach tests in parallel a set of alternatives and uses Friedman test to determine if an alternative is significantly worse than others.Random Forest and SMOTEnsemble generally perform best. race R package to perform F-racing algorithm https://cran.r-project.org/web/packages/race/index.html

- https://shiring.github.io/machine_learning/2017/04/02/unbalanced - Dealing with unbalanced data in machine learning

- Blagus, Rok, and Lara Lusa. “Class Prediction for High-Dimensional Class-Imbalanced Data.” BMC Bioinformatics 11 (October 20, 2010): 523. https://doi.org/10.1186/1471-2105-11-523. - Class imbalance empirical testing. Variable selection introduces additional bias towards classification into the majority class. Undersampling helps, oversampling does not. Variable normalization (centering) can worsen the performance.

- Bekkar, Mohamed, Hassiba Kheliouane Djemaa, and Taklit Akrouf Alitouche. "Evaluation measures for models assessment over imbalanced datasets." Iournal Of Information Engineering and Applications 3.10 (2013).  - List of various prediction metrics to use for imbalanced data and descriptions of each

- Whalen, Sean, Rebecca M. Truty, and Katherine S. Pollard. “Enhancer-Promoter Interactions Are Encoded by Complex Genomic Signatures on Looping Chromatin.” Nature Genetics 48, no. 5 (May 2016): 488–96. https://doi.org/10.1038/ng.3539.  -TargetFinder - ensemble learning of enhancer-promoter interactions within TADs using epigenomic data (DNAse hypersensitive sites, methylation, gene expression, TFBSs, conservation). Multiple machine learning techniques, applied to multiple cell types to learn shared and unique characteristics, as well to the combined cell line data modeled by epigenomic data common to the combined cell lines. Methylation, elongation histone modifications, AP-1 binding are best predictive features, not genes or conservation. Near optimal performance using 16 features. Importance of proximal features (distance). Introduction and discussion about previous use of epigenomic machine learning. https://github.com/shwhalen/targetfinder
- Xi, Wang, and Michael A Beer. “Local Epigenomic State Cannot Discriminate Interacting and Non-Interacting Enhancer-Promoter Pairs with High Accuracy,” October 25, 2018. https://doi.org/10.1101/420372. - Overfitting in epigenomics machine learning due to class imbalance. TargetFinder reanalysis. Gradient boosting, but not SVM, achieved good performance.

- Lu, Wei, Zhe Li, and Jinghui Chu. “Adaptive Ensemble Undersampling-Boost: A Novel Learning Framework for Imbalanced Data.” Journal of Systems and Software 132 (October 2017): 272–82. https://doi.org/10.1016/j.jss.2017.07.006. - Class imbalance problem, proposed the Ensemble of Undersampling (EUS) technique in combination with Real (in contrast to Discrete) Adaboost, cost-sensitive weighting, and adaptive boundary decision strategy. Introduction to the problem, references to existing methods. Tested on 18 datasets from KEEL repository, performance measured using AUC and other metrics.

- Dubey, Rashmi, Jiayu Zhou, Yalin Wang, Paul M. Thompson, Jieping Ye, and Alzheimer’s Disease Neuroimaging Initiative. “Analysis of Sampling Techniques for Imbalanced Data: An n = 648 ADNI Study.” NeuroImage 87 (February 15, 2014): 220–41. https://doi.org/10.1016/j.neuroimage.2013.10.005. - Class imbalance testing. Various sampling techniques (over-/undersampling, SMOTE), six feature selection algorithms, ensemble feature selection using RF and SVM. K-medoids undersampling is optimal. Description of existing approaches. Systematic framework to evaluate various combinations of F (feature selections algorithms) x S (set of class-imbalance handling approaches) x C ( classifiers). k-fold cross-validation to avoid bias due to random sampling.

- Blagus, Rok, and Lara Lusa. “SMOTE for High-Dimensional Class-Imbalanced Data.” BMC Bioinformatics 14, no. 1 (2013): 106. https://doi.org/10.1186/1471-2105-14-106. - SMOTE theoretical and empirical benchmarking. Inferior for high-dimensional data than undersampling. Class imbalance is most problematic in high-dimensional data. Variable reduction is needed before classification (ref 37). 

- Lunardon, Nicola, Giovanna Menardi, and Nicola Torelli. “ROSE: A Package for Binary Imbalanced Learning.” R Journal 6, no. 1 (2014). - ROSE - R package for over-/undersampling. Menardi and Torelli reference to read. https://cran.r-project.org/web/packages/ROSE/index.html

- Zhang, Ruochi, Yuchuan Wang, Yang Yang, Yang Zhang, and Jian Ma. “Predicting CTCF-Mediated Chromatin Loops Using CTCF-MP.” Bioinformatics 34, no. 13 (July 1, 2018): i133–41. https://doi.org/10.1093/bioinformatics/bty248. - CTCF-MP - boosted tree classifier for loop prediction from sequence-level features (word2vec, deep autoencoder to compress the 200-dimensional space to 32 dimensions, then tSNE mapping to 2D), CTCF ChIP-seq and DNAse-seq data. Convergent CTCF sites are important. Best prediction of common loops, little worse cell-type-specific. Imbalanced dataset performed well. https://github.com/ma-compbio/CTCF-MP

- Wong, Kin Yau, Cheng Fan, Maki Tanioka, Joe S. Parker, Andrew B. Nobel, Donglin Zeng, Dan-Yu Lin, and Charles M. Perou. “An Integrative Boosting Approach for Predicting Survival Time With Multiple Genomics Platforms,” June 4, 2018. https://doi.org/10.1101/338145. - Survival analysis using integrative approach - multiple data types (clinical, gene/protein expression, gene signatures) and methods (LASSO, elastic net, boosting using cross-validation and permutation). Good example of presenting comparison of methods on a biological problem.

- Mayr, A., H. Binder, O. Gefeller, and M. Schmid. “The Evolution of Boosting Algorithms. From Machine Learning to Statistical Modelling.” Methods of Information in Medicine 53, no. 6 (2014): 419–27. https://doi.org/10.3414/ME13-01-0122. - Boosting, part 1. AdaBoost, algorithm, overfitting and resistance to it, connection to generalized additive model (GAM). Likelihood-based boosting. Early stopping as a way to prevent overfitting. R packages mboost, gbm, GAMBoost, CoxBoost

- Haibo He, Yang Bai, Edwardo A. Garcia, and Shutao Li. “ADASYN: Adaptive Synthetic Sampling Approach for Imbalanced Learning,” 1322–28. IEEE, 2008. https://doi.org/10.1109/IJCNN.2008.4633969. - ADASYN - adaptive synthetic sampling approach. Overview of current methodologies - sampling strategies, synthetic data generation (SMOTE), others. Comparison with others. Open question - how much imbalance hurt learning? Important - categorical variables are excluded. https://cran.r-project.org/web/packages/smotefamily/

- Turki, Turki, and Zhi Wei. “Boosting Support Vector Machines for Cancer Discrimination Tasks.” Computers in Biology and Medicine 101 (October 1, 2018): 236–49. https://doi.org/10.1016/j.compbiomed.2018.08.006. - Boosting SVM outperforms regular SVM and xgboost. Three versions of the algorithm. Tested on cancer datasets. An example which metrics to use for classifier performance evaluation, and how to describe and present the results in tables (figures are bad).

- Drummond, Chris, Robert C Holte, and others. “C4. 5, Class Imbalance, and Cost Sensitivity: Why under-Sampling Beats over-Sampling.” In Workshop on Learning from Imbalanced Datasets II, 11:1–8, 2003. - a reference demonstrating that undersampling is better than oversampling in class imbalance problem

- Bekkar, Mohamed, Hassiba Kheliouane Djemaa, and Taklit Akrouf Alitouche. “Evaluation Measures for Models Assessment over Imbalanced Datasets.” Iournal Of Information Engineering and Applications 3, no. 10 (2013). - Classifier performance metrics (G-means, likelihood ratios, Discriminant power, F-measure, Youden index, MCC, ROC, (P/W)AUC and more), and their behavior in class imbalance settings. MCC is considered the best metric for imbalanced data learning.

- Galar, M., A. Fernandez, E. Barrenechea, H. Bustince, and F. Herrera. “A Review on Ensembles for the Class Imbalance Problem: Bagging-, Boosting-, and Hybrid-Based Approaches.” IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 42, no. 4 (July 2012): 463–84. https://doi.org/10.1109/TSMCC.2011.2161285. - Ensemble learning in class imbalance. Classifier metrics, from accuracy to AUROC. Three groups of approaches: algorithm-level (bias learning towards the minority class), data-level (resampling), cost-sensitive (mix). Resampling techniques also into three categories: RUS, ROS, SMOTE and variants, SPIDER. Overview of ensemble algorithms (Bagging, Boosting), classification of them to address class imbalance problem (Figure 3), description of each. Systematic experimental testing against C4.5 baseline using 44 KEEL datasets. SMOTEBagging, RUSBoost, UnderBagging are best.

- Lemaître, Guillaume, Fernando Nogueira, and Christos K. Aridas. “Imbalanced-Learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning.” Journal of Machine Learning Research 18, no. 17 (2017): 1–5. http://jmlr.org/papers/v18/16-365. - imbalanced-learning Python package. 18 techniques, Table. The class imbalance problem has been encountered in multiple areas such as telecommu- nication managements, bioinformatics, fraud detection, and medical diagnosis, and has been considered one of the top 10 problems in data mining and pattern recognition (Yang and Wu, 2006; Rastgoo et al., 2016). https://github.com/scikit-learn-contrib/imbalanced-learn

- Wei, Qiong, and Roland L. Dunbrack. “The Role of Balanced Training and Testing Data Sets for Binary Classifiers in Bioinformatics.” Edited by Iddo Friedberg. PLoS ONE 8, no. 7 (July 9, 2013): e67863. https://doi.org/10.1371/journal.pone.0067863. - Training set should be balanced, tested set stays as-is, unbalanced.


## Misc

- Idea that TAD boundaries may not be created equal. Huang, Jialiang, Kailong Li, Wenqing Cai, Xin Liu, Yuannyu Zhang, Stuart H. Orkin, Jian Xu, and Guo-Cheng Yuan. “Dissecting Super-Enhancer Hierarchy Based on Chromatin Interactions.” Nature Communications 9, no. 1 (05 2018): 943. https://doi.org/10.1038/s41467-018-03279-9. - Enhancers can be separated into super- (SE) and regular (RE) enhancers, SE further classified into hub and nonhub based on Hi-C interaction frequency. ROSE to identify enhancers from H3K27ac peaks. Characterization of SEs  - broader. Characterization of hubs - enriched in CTCF, cohesin, disease-associated variants, 

- CTCF is not at TAD boundaries, but nearby. "only 15% CTCF sites are at the TAD boundary while 85% CTCF sites are inside the TAD [@Ong:2014aa]."

- CTCF showed a remarkable enrichment immediately outside of these boundaries, with sites on the plus strand sharply peaking at upstream TAD boundaries and those on the minus strand peaking at downstream boundaries. Similar enrich- ments at TAD boundaries are observed for RAD21, SMC3 (Cohesin complex), and ZNF143, consistent with previous reports [@Dixon:2012aa][@Zuin:2014aa][@Bailey2015].