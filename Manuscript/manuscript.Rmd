---
title: "Predictive modeling using genomic annotations"
subtitle: "Pitfalls and Recommendations"
author: "Spiro Stilianoudakis"
date: "August 9, 2018"
header-includes:
    - \usepackage{setspace}\doublespacing
    - \usepackage{amsmath}
output: 
  pdf_document:
    toc: yes
  word_document:
    fig_caption: yes
    fig_height: 3
    fig_width: 2
    toc: yes
bibliography: C:/Users/Spiro Stilianoudakis/Documents/TAD_predictive_modeling/Manuscript/References
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# ABSTRACT

**Background.** Chromosome conformation capture sequencing technologies have shown that the three-dimensional (3D) structure of the genome is folded into distinct compartments, known as topologically associated domains (TADs) - units of coordinated gene expression. The location of TAD boundaries is highly conserved, suggesting the presence of epigenomic marks aiding in TAD formation. The ability to predict which epigenomic features are most associated with the TAD boundaries will allow to better understand the regulatory role of the 3D structure of the genome.

Existing methods for predicting associations between genomic elements tend to ignore key characteristics of the genomic data. Specifically, the number of TAD boundaries is much less than the number of other genomic regions, leading to heavily imbalanced classes. Furthermore, most methods utilize direct overlap as a means to quantify the association, while distance, the measure of spatial relationships, remains unaccounted for. Consequently, distances on a genomic scale vary widely, leaving uncertainty how the heavily right-tailed distribution of distance measures will affect the model’s performance.

**Methods.** We propose a novel data pre-processing pipeline that addresses those shortcomings. A number of classifier performance metrics were assessed, including the F1 measure and Matthew Correlation Coefficient (MCC), and area under the ROC curve (AUROC).

**Results.** Data preprocessing (log2-transformation and standardization) improves the performance of classification algorithms and allows for the ability to more accurately predict which genomic features are most associated with TAD boundaries. 

**Conclusions.** Current methods used to model the epigenomic features associated with TAD boundaries are insufficiently robust to handle properties of genomic data. Models applied to unprocessed data can have poor predictive performances. Focusing solely on typical performance assessment metrics, such as AUROC, can mask poor performance of the models; thus, the use of more balanced metrics, such as F1 and MCC, is warranted. Our model results in better performances and more accurate identification of important features associated with the formation of TAD boundaries.

# INTRODUCTION

The advent of various genome-wide sequencing technologies, such as high-throughput conformation capture (notably Hi-C), have revealed how the spatial organization of the human genome may affect several epigenetic functions [-@lieberman2009comprehensive]. Analyses have shown that the genome is tightly compacted into distinct compartments.  There exist regions within these compartments that have been shown to be highly conserved and self-interacting, and termed topologically associating domains (TADs) [-@dixon2012topological]. Evidence suggests that regulatory elements and genes tend to interact more frequently within the same TAD [-@symmons2014functional]. This suggests that the boundaries of TADs may play a role in restricting the function of certain genomic elements such as enhancers, thereby impacting the transcription of genes. Furthermore, changing the 3D structure of the DNA, causing disruptions in these domains can lead to adverse outcomes and diseases like cancer. Therefore, it has become increasingly important to be able to identify the key molecular drivers of the formation of TAD boundaries in order to further our understanding of the human genome.

The mechanisms underlying the formation of TADs are a complex and active area of research. Recently, it has been discovered that insulator sequences have a primary role in orchestrating the topological arrangements of higher-order chromatin architecture [-@phillips2013chromatin]. Insulators are multi-faceted regulatory sequences that moderate a variety of genomic processes including activation, repression, and enhancer blocking. Specifically, the insulator binding protein CTCF has been found to be enriched at the boundary sequences of topologically associating domains in human cells and may therefore act as a mediator of long-range chromatin contacts [-@zuin2014cohesin]. Other regulatory elements like particular histone modifications such as H3k36 and H3K27 trimethylation, and transcription factors such as ZNF274 and YY1 have been more frequently observed at TAD boundaries than in other regions [-@rao20143d]. These have been found to be associated with more open chromatin which alter the accessibility of genes for transcription. The distinct patterns of some of these different proteins and functional elements point toward the opportunity of using computational approaches in identifying which epigenomic features are most predictive of the development of TAD boundaries. This will allow us to better understand what leads to their formation.

Most accepted methods for predicting TAD boundaries consider performing a classification algorithm by binning the genome into equal-sized bins and labeling them as either containing a TAD boundary or not. However, a pervasive issue becomes the introduction of highly imbalanced classes. Due to the sparcity of TADs throughout the genome, most bins will not contain a TAD boundary. Therefore, the minority class of genomic bins with TAD boundaries in them becomes proportionally smaller than that of the majority class of genomic bins with no TAD boundaries. It is commonly agreed upon in the machine learning community that imbalanced datasets adversely impact the performance of the classifiers as the learned model is biased towards the majority class to minimize the overall error rate [-@estabrooks2004multiple].  However, a clear method for creating balanced classes remains evasive and largely dependent on the problem at hand. For that reason we evaluated 3 widely accepted class balancing techniques, in addition to no balancing. They included random under-sampling, random over-sampling, and synthetic minority over-sampling technique (SMOTE). 

Model performance is also contingent upon the resolution of the contact matrix used to identify the location of TADs throughout the genome. This will also affect the size of the genomic bins used in model construction. A larger resolution contact matrix will result in the identification of larger-sized TADs, and subsequently, less TAD boundary points. This will contribute to smaller sample sizes and could effect predictive performances.

[Mention Feature Engineering]

Another problem lies in identifying the feature space of subsequent classification algorithms.  A variety of positional characteristics of the genomic features can be used to describes the relationship between TAD boundaries and genomic elements, but it is unclear which is best. The use of distances (in base pairs) from genomic bins to regions of functional genomic elements has largely been ignored. However, these distance characteristics offer a more accurate spatial representation of which genomic features are associated with TAD boundaries.

Due to the increasingly large amounts of functional genomic elements to consider, it is a popular approach in genomic studies to use variable reduction techniques to reduce the noise induced by noninformative predictors and increase computational efficiency. Two popular approaches use the $l_{1}$ norm (LASSO) and a combination of the $l_{1}$ and $l_{2}$ norms together (Elastic-Net). While both regularization techniques have merit, LASSO’s inability to handle correlated predictors poses a problem when identifying which element are most predictive of TAD boundaries. The elastic-net, however, is better suited in a situation where multiple genomic elements colocalize around the same TAD boundaries, and could be better suited for this type of alaysis [-@zou2005regularization].

Careful consideration must also be taken when deciding which performance metric(s) to present when evaluating predictive models. A common metric to consider is the the area under the ROC curve (AUC), as well as others including accuracy, sensitivity, and specificity. 

The popularity surrounding the use of ROC curves is that they do not depend on a threshold. However, when the prevalence of an event is low--that is, a genomic bin containing a TAD boundary--mean probabilities are biased towards the majority class. Likewise, metrics like accuracy and sensitivity and specificity do not incorporate the entirety of the confusion matrix created when validating predictive models on a test data set. Therefore, each of these metrics are likely to be inflated when evaluating model performance on imbalanced data. More robust metrics like the F1-score and Matthew's Correlation Coefficient are more suitable for this type of analysis and can highlight problems associated with applying predictive models on imbalanced data.

[Mention Previous Methods]

In this study we have developed an ensemble framework that aims to address and compare a variety of data irregularities and characteristics that are associated with the prediction of TAD boundaries. We evaluated domain data obtained from four different resolution contact matrices for the GM12878 cell line including 5kb, 25kb, 50kb, and 100kb. For each resolution, we examined the inclusion of three different predictor types as the feature space for downstream modeling. We coupled each predictor type with two separate variable reduction techniques. Lastly, we performed a series of three different re-sampling techniques aimed at created balanced classes. Random forest classification algorithms were performed to both compare each combination of the above ensemble, and investigate which functional genomic features were most associated with the formation of TAD boundaries.  Model performance was evaluated based on a total of four metrics which included test accuracy, area under the ROC curve (AUC), F1 score, and Matthew's Correlation Coefficient (MCC). [mention reasons for including 4 metrics]

#METHODS

##DOMAIN DATA

Topologically associating domain data for the GM12878 cell line was obtained using the arrowhead algorithm from the Juicer toolbox (https://github.com/aidenlab/juicer). Publically available domain data was obtained from in situ Hi-C contact matrices at 5kb resolution [-@rao20143d]. Additional domain data was obtained from Dali et al. at 25kb, 50kb, and 100kb resolutions [-@dali2017critical]. 

Identified TAD boundaries were represented by their location in the genome (hg19 human genome assembly), including chromosome, start and end coordinates for chromosomes 1 through 22. The start and end coordinates were concatenated, sorted, and unique coordinates representing the borders demarcating TADs were obtained.

Figure 1 presents a diagram of how the data was established for modeling purposes. First, the genome was binned into equally sized intervals. This was performed at the same resolution of the contact matrix that the domain data was obtained from. For example, for the domain data obtained from a 5kb resolution contact matrix, the genome was subsequently binned into 5kb intervals. The same processes was repeated for each of the other three resolutions. Next, the boundary coordinates were flanked on either side by a unit corresponding to the resolution of the particular domain data. Again, siting the domain data obtained from a 5kb resolution contact matrix, each uniquely identified TAD boundary was flanked on either side by 5kb for a total width around each boundary point of 10kb. The same process was repeated for the other three resolutions. A genomic bin was labeled as having a TAD boundary within it (Y=1) if it overlapped with a particular flanked boundary. Otherwise, the bin was labeled as not containing a boundary (Y=0).

##GENOMIC ANNOTATIONS

Annotation data, in the form of functional genomic elements from ChIP-seq experiments were obtained from the Encyclopedia of DNA Elements (ENCODE) Consortium. The annotations consisted of a variety of different chromatin states, histone marks, and DNase I hypersensitive sites. Table 1 presents the total list of genomic  annotations considered in this study and the sources of downloadable files. Each annotation was represented by their location throughout the genome via chromosomal coordinates. An example of a specific annotation, which is labeled as a functional genomic element, is presented in Figure 1. It is represented by the green segments that overlap with the genomic bins. 

##Feature Engineering

Using each genomic element, we created three types of predictors that were used as features to analyze their relationships with TAD boundaries in downstream modelling. Figure 2 presents an illustration of each type of predictor that was considered. The predictors consisted of two different types of overlaps, and a distance type. They are described in detail below:

   1.	Overlap Counts (OC): For each genomic bin, the total number of overlapped regions between a functional genomic element and a bin was calculated. This was defined as the overlap counts (OC) and is illustrated in the left part of Figure 2. Within each bin (marked by the blue dashed lines), the total number of overlapped regions of the genomic element (represented by the horizontal green segments) are tallied and marked below. In the case of no overlaps the value is given as 0.
   
   2.	Overlap Percent (OP): For each genomic bin, the percent overlap between regions defined by a particular functional element and a bin was calculated. From the middle diagram of Figure 2, the feature width was defined as the number of base pairs that overlapped with the genomic bin. The percent was then calculated by dividing the feature width with the bin width. If multiple overlaps existed, the feature width was defined as the sum of the total number of base pairs that overlapped with a particular bin. Note that the bin width remained constant for a given resolution (either 5kb, 25kb, 50kb, or 100kb). Similar to the OC predictor type, bins with no overlaps were given to be 0.

   3.	Distance: For each genomic bin, the distance (in base pairs) from the center of the bin to the center of the nearest region of a respective functional element was calculated. The right most diagram of Figure 2 depicts an example of how distance was calculated. The vertical green segments represent the centers of the respective genomic element. Each distance, $D_{i}$, represented by the blue brackets are the number of base pairs from the center of the bins to the center of the nearest genomic elemental region.
   
[Mention transformations used]

##ENSEMBLE FRAMEWORK

Figure 3 details the ensemble framework proposed and evaluated in this study. For each resolution and predictor type chosen, the data was split into training to testing sets. A variable reduction technique was then performed. The feature space was reduced and a resampling technique was applied. Lastly, a classification algorithm in the form of a random forest was performed and validated. Details of each step in the framework are provided below.

###Training and Testing Sets

Given a particular resolution, the full data set was composed of genomic bins that either overlapped with a flanked TAD boundary (minority set) or did not (majority set). Figure 3 highlights the severe class imbalance present in both the full data set. The data was then split into a 7:3 ratio of training to testing sets. Each training and testing set was composed of a similar majority to minority class ratio as the full data set, which is represented by the segmented regions in Figure 3. The training set was then used to perform all downstream modeling. The same test set was held separate and used to validate all models.

###Variable Reduction

We evaluated the use of two variable reduction algorithms, the LASSO and Elastic-Net regularizations. Each algorithm was performed prior to re-sampling. For the LASSO, the $\lambda$ penalization term was tuned over a grid of 10 values on an exponential scale ranging from 0.01 to 10. The additional $\alpha$ term for the Elastic-Net was tuned over 10 equa-distant values from 0 to 1.

###Re-Sampling Techniques

After choosing one of the aforementioned data resolutions, predictor types, and variable reduction algorithms, we then evaluated three re-sampling techniques used to create balanced classes.  For each resampling technique, the majority class is defined as the set of genomic bins that did not include a TAD boundary, while the minority class is defined as the set of bins that included a TAD boundary. The following details the re-sampling techniques considered in this paper:

   1.	No Sampling: All of the data points from the majority and minority class of the training set were used.

   2.	Random Under-Sampling: All of the minority classes from the training set were used. A random set without replacement from the majority classes of the training set were used to match the number of minority samples. In this paper we performed 50 iterations of random under-sampling. At each iteration a random forest classification algorithm was performed. Performance metrics were aggregated by taking the average across all of the iterations.

   3.	Random Over-Sampling: All of the majority classes from the training set were used. A random set with replacement from the minority classes of the training set were used to match the number of majority samples. Similar to the under-sampling technique, 50 iterations were performed and results were aggregated by taking the average across iterations.

   4.	SMOTE (Synthetic Minority Over-Sampling Technique): This method incorporated both random under- and over-sampling. Under-sampling is performed without replacement from the majority class, while over-sampling is performed by creating new synthetic observations using the minority class. The SMOTE algorithm was devised by Chawla et al. [-@chawla2002smote] and stored in the DMwR package, available in R. The user is able to specify the parameters denoting the percent of over- and under-sampling from the two classes. This allows the user to control the proportion of disbalance in the data. For the purposes of this paper, we used SMOTE to create perfectly balanced data. However further research would be needed to see if varying the proportion of class disbalance could improve predictive performance.

###Classification

A classification algorithm in the form of a random forest was performed and validated using the test set. 10-fold cross validation was used to reduced bias due to random dataset generation. The default number of features to consider at each node of the random forest algorithm was set as the square root of the number of features in the model. Likewise, the number of ensemble trees to aggregate was set at 500. Once the model was implemented, it was assessed using the test set, and performance metrics were then recorded. 

Using set notation the formulation of the ensemble framework can be defined as follows:

Set of different resolutions:

R = {5kb, 25kb, 50kb, 100kb}

Set of Predictor Types:

P = {OC, OP, Distance}

Set of Variable Reduction Techniques:

V = {LASSO, Elastic-Net}

Set of Class-Balancing Methods:

C = {None, RUS, ROS, SMOTE}

Therefore, a system produced from the ensemble framework is defined as follows:

E = {r, p, v, c}, where r in R, p in P, v in V, and c in C.

Once a system was defined, a random forest algorithm was performed. Letting |X| denote the cardinality of each set, then we evaluated |R| x |P| x |V| x |C| different ensemble systems in this paper.

###Model Evaluation

The efficacy of different ensemble systems was compared using various performance metrics including accuracy, AUC, F1-score, and MCC. These metrics are defined as follows:

$$
\begin{equation}
\begin{aligned}
Accuracy &= \dfrac{TP + TN}{TP + TN + FP + FN} \\
F1 - Score &= \dfrac{2TP}{2TP + FP + FN} \\
MCC &= \dfrac{TP \times TN - FP \times FN}{\sqrt{ (TP + FP)(TP+FN)(TN+FP)(TN+FN) }}
\end{aligned}
\end{equation}
$$
Here, TP refers to the number of bins correctly identified as containing a TAD boundary (true positives), FP refers to the number of bins incorrectly identified as containing a TAD boundary (false positives), TN refers to the number of bins correctly identified as not containing a TAD boundary (true negatives), and FN refers to the number of bins incorrectly identified as not containing a TAD boundary. Each of these quantities are obtained from the confusion matrix created by validating the model on the testing set. Accuracy measures the percentage of correct classifications by the model. However, in the case of heavily imbalanced class, as can be seen by the numerator, the metric is biased toward correcly identified majority classes. The F1-score and MCC are more balanced measures. We also considered the AUC for each model, by computing the average trapezoidal approximations for the curve created by the true positive rates (TPR) and false positive rates (FPR)

#RESULTS

##ENSEMBLE COMPARISONS

In this section we provide the details of the comprehensive experiments performed in order to assess each ensemble system. For the task of predicting TAD boundaries using functional genomic elements, we evaluated 4 domain data sets obtained using the arrowhead algorithm for the Juicer toolbox developed by Rao et al, 3 predictor types, 2 variable reduction techniques, and 4 class balancing techniques. We evaluated each ensemble system using 10-fold cross-validated random forest models. Thus, we generated 96 (=4x3x2x4) separate models.

After binning the genome at 5kb, 25kb, 50kb, and 10kb intervals there were a total of 44948, 9042, 4541, and 2279 genomic bins created respectively. Figure 4 illustrates the class imbalance problem present in each data resolution. Likewise, we see the apparent decrease in data points (bins) as resolution increases. Furthermore, Table 1 shows that for each class balancing technique, perfectly balanced classes were created.

###DISTANCE PREDICTORS IMPROVE MODEL PERFORMANCE IN ALL RESOLUTIONS

Figures 4-7 present the results across all of the ensemble systems for each resolution of domain data. Focusing on the top row in each of Figure 4-7, indicating no balancing, we see that the model using distance predictors, represented by the blue bars, outperformed models using the other two predictor types. However, the disparity become less evident as the resolution increased. In each of the 4 figures we see that the F1-score and MCC metrics were noticeably lower than the accuracy and AUC. This highlights the problems associated with applying predictive models to unbalanced data.

###CLASS BALANCING IMPROVES MODEL PERFORMANCE IN ALL RESOLUTIONS

Moving down the rows in each of Figures 4-7 we see that model performance was markedly improved among the F1-scores and MCCs after creating balanced classes compared to the results using no re-sampling. For each of the resolutions, we see that random under-sampling and SMOTE re-sampling techniques performed very similarly, while random over-sampling performed poorly in comparison. Furthermore, we see that the elastic-net reduction technique outperformed the LASSO. Although, the disparity became less evident as the resolution increased. Specific ensemble results can be found in Supplement Tables X-X.

###PERFORMANCES BECOME INVARIANT AS RESOLUTION INCREASES

Figure 8 compares the results of random forest models using distance-type predictors, with elastic-net regularization, over 50 iterations of random under-sampling, across each domain data resolution. We see that the results for F1-scores are relatively stable between 5kb, 25kb, and 50kb resolutions with a sharp decrease for the 100kb resolution. Furthermore, the decrease in performance is more evident and steady across accuracy, AUC, and MCC metrics.

To identify the functional genomic elements that were most predictive of TAD boundaries, we analyzed the variable importance produced by each random forest algorithm. Figure 9 presents the scaled importance plots of the top 15 most predictive genomic elements for each resolution. We see that for the 5kb resolution, the most predictive elements include insulator proteins, DNase hypersensitive sites, the transcription factor CTCF, as well as histone modifications H3k9ac, H2az, and H3k4me2. These results align with what is seen in the literature. Although, the insulator protein is featured as a top predictor in 2 of the other 3 resolutions, the rest of the predictors are less consistent among each other and less in agreement with the literature. The results from Figure 8 and 9 confirm that the model results appear to degrade as resolution increases with respect to both prediction and the identification of genomic elements associated with TAD boundaries.




