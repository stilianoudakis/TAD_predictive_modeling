---
title: "Computational Prediction of Boundaries of 3D Genomic Domains in Class Imbalance Settings"
csl: styles.ref/genomebiology.csl
output:
  word_document:
    reference_docx: styles.doc/NIH_grant_style.docx
bibliography: References/references.bib
--- 

Spiro C. Stilianoudakis^1^ (stilianoudasc@mymail.vcu.edu), Mikhail G. Dozmorov^1\*^ (mikhail.dozmorov@vcuhealth.org)

^1^ Dept. of Biostatistics, Virginia Commonwealth University, Richmond, VA, 23298, USA  
^\*^ To whom correspondence should be addressed: Virginia Commonwealth University, Richmond, VA, 23298, 804-827-2055, mikhail.dozmorov@vcuhealth.org

# ABSTRACT

**Background.** Chromosome conformation capture sequencing technologies have shown that the three-dimensional (3D) structure of the genome is folded into distinct compartments, known as topologically associated domains (TADs) - units of coordinated gene expression. The location of TAD boundaries is highly conserved, suggesting the presence of epigenomic marks aiding in TAD formation. The ability to predict which epigenomic features are most associated with the TAD boundaries will allow to better understand the regulatory role of the 3D structure of the genome.

Existing methods for predicting associations between genomic elements tend to ignore key characteristics of genomic data. Specifically, the number of TAD boundaries is much less than the number of other genomic regions, leading to heavily imbalanced classes [??? and why this is the problem? Instead of classes, something like "leading to the model biased towards classifying the majority of the regions". Rewrite]. Furthermore, most methods utilize direct overlap as a means to quantify the association, while distance, the measure of spatial relationships, remains unaccounted for. Consequently, distances on a genomic scale vary widely, leaving uncertainty how the heavily right-tailed distribution of distance measures will affect the modelâ€™s performance.

**Methods.** We propose a novel data pre-processing pipeline that systematically addresses those shortcomings. Lasso and Elastic Net [??? Adjust methods to the current setup] were used for feature selection, coupled with five classifier methods. A number of classifier performance metrics were assessed, including the F1 measure and Matthew Correlation Coefficient (MCC), and area under the ROC curve (AUROC).

**Results.** Data preprocessing (log2-transformation and standardization) improves the performance of classification algorithms and allows for the ability to more accurately predict which genomic features are most associated with TAD boundaries. In contrast to overlap-based association measures, the distance between genomic elements was the most predictive measure. The random undersampling strategy addresses the class imbalance problem the most effectively in nearly all settings.

**Conclusions.** Current methods used to model the epigenomic features associated with TAD boundaries are insufficiently robust to handle properties of genomic data. Models applied to unprocessed data can have poor predictive performances. Focusing solely on standard performance assessment metrics, such as AUROC, can mask poor performance of the models; thus, the use of more balanced metrics, such as F1 and MCC, is warranted. Our model results in better performances and more accurate identification of important features associated with the formation of TAD boundaries.

# INTRODUCTION

The advent of various genome-wide sequencing technologies, such as high-throughput conformation capture (notably Hi-C), have revealed how the spatial organization of the human genome may affect genomic regulation [@lieberman2009comprehensive]. Analyses have shown that the genome is folded into distinct compartments termed topologically associating domains (TADs) [@dixon2012topological]. Evidence suggests that regulatory elements and genes tend to interact more frequently within the same TAD [@symmons2014functional]. Approximately 60-80% of TADs remain stable across cell types [@Schmitt:2016aa], suggesting that the boundaries of TADs may play a role in restricting the function of certain genomic elements such as enhancers, thereby impacting the transcription of genes. Furthermore, studies have shown that changing the 3D structure of the DNA, causing disruptions in these domains, can lead to adverse outcomes and diseases like cancer [@hnisz2016activation; @flavahan2016insulator]. Therefore, it has become increasingly important to be able to identify the key molecular drivers of the formation of TAD boundaries in order to further our understanding of the human genome.

The distinct patterns of epigenomic marks, binding peaks of different factors, and actively transcribed genes prompt the use of computational approaches to identify which epigenomic features are most predictive of TAD boundaries. Current methods rely on the use of classification algorithms to draw associations between the spatial relationship of functional genomic elements and regions that include TAD boundaries. Although several useful insights have been made, such as the convergent binding of CTCF [@ong2014ctcf; @ghirlando2016ctcf], many methods ignore key characteristics of domain data, such as the impact of heavily imbalanced number of boundaries vs. other regions. In a classification setting, when one is attempting to discriminate between regions with and without TAD boundaries, the class imbalance issue will contribute to small imbalance ratios (defined as the ratio of minority to majority classes) [@Orriols-Puig2008]. Such heavily imbalanced datasets are known to adversely impact the performance of the classifiers as the learned model is biased towards the majority class to minimize the overall error rate [@lusa2010class;@chen2013influence;@jeni2013facing]. Consequently, careful consideration must be taken when deciding which performance metric(s) to use for evaluation of predictive models, especially in the case of imbalanced data. 

Commonly used measures of model performance for binary classifiers include threshold-free metrics like the area under the receiver operating curve (AUROC), and other threshold-specific metrics like accuracy, sensitivity, and specificity. The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The subsequent AUROC, measures how well the model is capable of distinguishing between the two classes [@park2004receiver]. Accuracy is defined as the ratio of correctly identified cases (either from the positive or negative class) over the total number of cases. It measures the overall proportion of correctly classified instances. Sensitivity, also referred to as recall or TPR, is the ratio of true positives over the total number of positive classes. Specificity, on the other hand, also referred to as TNR, is the ratio of true negatives over the total number of negative classes. Each measures the proportion of correctly identified positive and negative classes respectively [@yerushalmy1947statistical]. Problems arise when using such measures in the event of imbalanced classes. For AUROC, when the prevalence of an event is low--that is, a defined genomic region truly containing a TAD boundary--mean probabilities are biased towards the majority class. This results in the area of interest under an ROC curve to be compressed toward a small corner in the ROC space [@prati2011survey]. While not directly affecting the value of the AUROC, this can have severe consequences on the interpretability of the model's performance. A better threshold-free measure to use when dealing with imbalanced data is the area under the precision-recall curve (AUPRC).  The interpretation of the AUPRC of a model is similar to that of the AUROC. However, where as the baseline model performance according to the AUROC is given by 0.50, the baseline model performance according to the AUPRC is given by the ratio of true positives over the total number of cases. This enables a more realistic benchmark to evaluate a model on as this ratio will be lower than 0.5 for a test data set with imbalanced classes. Likewise, threshold-specific metrics like accuracy and specificity are also problematic when evaluating model performance on imbalance data due to being highly influenced by the correct classification of the majority class. Certain composite threshold-specific metrics such as balanced accuracy and Youden's Index should be preferred because they soften the influence of a model's ability to correctly classify majority classes and yield more dependable and interpretable results [@jeni2013facing]. Here, balanced accuracy measures the average accuracy obtained on either class [@brodersen2010balanced]. Informedness, also referred to as Youden's Index, measures the probability of the model making an informed decision and corresponds to giving equal weight per class, rather than to each instance like Accuracy [@youden1950index;@powers2011evaluation].

<!--
Better measures to consider are composite metrics involving both precision and recall together. Therefore, measures like the F1-Score, Youden's Index (YI), and Matthew's Correlation Coefficient (MCC) offer better interpretation, especially in the instance of imbalanced classes [@jeni2013facing]. The F1-Score can be interpreted as a weighted average of the precision and recall values, while Youden's index is interpreted as the arithmetic mean between sensitivity (recall) and specificity. MCC is, in essence, the correlation coefficient between the observed and predicted binary classifications. Each metric uses all 4 quadrants of the confusion matrix, thus providing more balanced measures in the case of imbalanced data [@chicco2017ten].
-->

In addition to employing suitable metrics for evaluating models validated on imbalanced data, there also exist several algorithmic or data level solutions. Algorithmic level solutions involve modifyng existing classifiers in order to curtail the bias induced by imbalanced classes [@estabrooks2004multiple; @chawla2003smoteboost; @chen2004using; @khoshgoftaar2007empirical]. Data level solutions, on the other hand, mostly constitute as resampling techniques, in which random sampling is used to either replicate or remove data points in order to create a more balanced dataset [@chawla2004special; @chawla2009data]. It has been shown that quite simple resampling techniques can drastically improve model performance in the face of imbalanced data [@japkowicz2002class; @dubey2014analysis]. For the purposes of this study, we focus our attention toward data level solutions so that all results may be classifier-independent, and, thus, more easily comparable. 

Another aspect of TAD boundary modelling not currently being explored are the various feature engineering methods used to build the feature space of models. Different techniques can be employed to measure the relationship between TAD boundaries and genomic annotations. Typical approaches for modeling the association between TAD boundaries and epigenomic elements include overlap counts or percents [@mourad2016computational; @hong2017computational; @huang2015predicting]. Counts refer to the total number of instances where a genomic annotation overlaps with a genomic region of interest, while percents refers to the percent of overlap that occurred (usually in terms of base pairs). The use of distances from TAD boundaries to regions of various genomic features, measured in the number of bases between them, has largely been ignored. However, these distance characteristics have the potential to offer a more accurate spatial representation of which genomic features are associated with TAD boundaries. Likewise, any direct comparisons of the different feature engineering techniques remain unresearched.

Additionally, model performance is heavily contingent upon the resolution of the contact matrix used to identify the location of TADs throughout the genome. It is often unclear from previous studies what resolution contact matrices were used when retrieving TAD boundaries, and how other resolutions would affect their results. A lower resolution contact matrix will result in the identification of larger-sized TADs, and subsequently, less TAD boundary points. This will contribute to smaller sample sizes and could effect predictive performances. 

In this study, we have developed an ensemble framework for predicting TAD boundaries in order to systematically compare many data driven solutions used to address the issues presented above. We evaluated Hi-C data for both the GM12878 and K562 cell line obtained at 10 kb, 25 kb, 50 kb, and 100 kb resolution. For each resolution, we examined the inclusion of three different predictor types, including distance as the measure of association between genomic elements. We then tested four different re-sampling techniques aimed at creating balanced classes. Noninformative features were removed using a feature reduction algorithm and random forest classifications were performed to assess model performance, and investigate which genomic annotations were most associated with the TAD boundaries for a given system from the complete ensemble.  Model performance was evaluated using area under the precision-recall curve (AUPRC), F1-Score, Youden's index (YI), and Matthew's correlation coefficient (MCC). 

# METHODS

## DOMAIN DATA

Replicate Hi-C data for the GM12878 and K562 cell lines were downloaded from GEO GSE63525 [@rao20143d] (Supplementary Table 4). The autosomal genomic coordinates (GRCh37/hg19 human genome assembly) of TAD boundaries were obtained from data at 10 kb, 25 kb, 50 kb, and 100 kb resolution using the Arrowhead algorithm [@durand2016juicebox]. The start and end coordinates of all TADs were concatenated, sorted, and unique TAD boundary points were obtained. 

## GENOMIC ANNOTATION DATA

Genomic annotation data (chromatin states, histone marks, and transcription factor binding sites, GRCh37/hg19 human genome assembly) were obtained from the UCSC Genome Browser Database [@Tyner:2017aa] (Supplementary Table 1).


## MODEL CONSTRUCTION

To establish the data for modeling, the genome was binned into equally sized intervals defined by the data resolution for the corresponding contact matrix (Figure 1). For example, for domain data obtained from a 10 kb resolution contact matrix, the genome was binned into 10 kb intervals. For the boundary data, in order to accommodate for the uncertainty in exact boundary location, the boundary points were flanked on both sides by the size of the resolution of the corresponding contact matrix. For example, for 10 kb data, each uniquely identified TAD boundary was flanked on either side by 10 kilobases for a total width around each boundary point of 20 kb. A genomic bin was labeled as having a TAD boundary within it (Y=1) if it overlapped with a particular flanked boundary. Otherwise, the bin was labeled as not containing a boundary (Y=0).

## ENSEMBLE FRAMEWORK

An ensemble of models was established for the purposes of systematically comparing the performances of different parameters in the framework (Figure 2). Each system in the framework is assumed to be composed of a model constructed from a particular data resolution, with a particular predictor type for the feature space.

The data sets used for modeling were composed of the full set of genomic bins that either overlapped with a flanked TAD boundary or did not. These two classes made up the minority and majority sets respectively. The data was then split into a 7:3 ratio of training to testing sets. Each training and testing set was composed of a similar majority to minority class ratio as the full data set. For the task of performance evaluation and prediction, the test set remained constant for all ensemble systems to account for an unbiased comparison.

### FEATURE ENGINEERING AND RE-SAMPLING

Using the sets of genomic annotations, we created three types of predictors that were used as features for subsequent models to analyze their respective spatial relationship with TAD boundaries (Figure 3):

1. **Overlap Counts (OC)**: For each genomic bin, the total number of instances (bases) where a bin overlapped with genomic annotation regions (features) was calculated. In the case of no overlaps the value is set as 0.
   
2. **Overlap Percent (OP)**: For each genomic bin, the percent overlap between the feature width and the total width of the bin was calculated. We defined the feature width as the number of bases that overlapped with the genomic bin. The percent was then calculated by dividing the feature width by the bin width. If multiple overlaps existed, the feature width was defined as the sum of the total number of bases that overlapped with a particular bin. Note that the bin width remained constant for a given resolution (either 10 kb, 25 kb, 50 kb, or 100 kb). Values for bins with no overlaps were set to be 0.

3. **Log2 Distance**: For each genomic bin, the $log_{2}$ of the distance in bases (refered to hereafter simply as $\textit{distance}$), from the center of the bin to the center of the nearest region defined by a specific genomic annotation was calculated. The log transformation was used as a normalization technique in order to account for the skewness of the data (Supplementary Figures 1-8).

In order to assess the influence of imbalanced classes, we then evaluated four re-sampling techniques (in addition to no re-sampling), composed of data from each predictor type. Each re-sampling technique is described in detail below.

1. **No Sampling**: All of the data points from the majority and minority class of the training set were used.

2. **Random Under-Sampling (RUS)**: All of the minority classes from the training set were used. Sampling without replacement was used to obtain the same number of the majority classes. 

3. **Random Over-Sampling (ROS)**: All of the majority classes from the training set were used. Sampling with replacement was used to obtain the same number of the minority classes. 

4. **SMOTE** (Synthetic Minority Over-Sampling Technique): This method incorporates both random under- and over-sampling. Under-sampling is performed without replacement from the majority class, while over-sampling is performed by creating new synthetic observations using the minority class [@chawla2002smote]. The `DMwR` R package was used to perform SMOTE (version 0.4.1). 

<!--
5. **ROSE** (Random Over-Sampling Examples): Similar to SMOTE, ROSE combines techniques of over-sampling and under-sampling. However, contrary to SMOTE, all of the data generated by ROSE is synthetic. The data is created by sampling from one of the two classes, then generating a new example in its neighborhood using a smoothed bootstrap approach. The shape of the neighborhood is determined by the kernel density estimate of the two classes [@lunardon2013r]. Another important distinction between ROSE and SMOTE is that, for ROSE, the creation of perfectly balanced classes is unlikely due to the randomness in sampling between the two classes. The `ROSE` R package was used to perform ROSE (version 0.0.3).
-->

### FEATURE REDUCTION

A regularization technique using a combination of the $l_{1}$ and $l_{2}$ penalties, known as the elastic-net, was used to reduce the feature space. The elastic-net was chosen in order to suitably reduce the feature space while also maintaining the inclusion of multiple correlated predictors. For each elastic-net regularization, 10-fold cross-validation was used to tune the parameters that controlled the penalty terms. All elastic-net algorithms were implemented in R using the `caret` package version 6.0 [@kuhn2012caret].

<!--
Recursive feature elimination (RFE) was then used to determine the optimal number of features for models with the best performing predictor type. For each set of predictor type, random forest models were performed recursively on subsets of the feature space. The specific subsets of features were chosen as powers of 2 for computational efficiency. Model performance for each subset was evaluated using AUPRC. The optimal set of featues to include in downstream analyses was determined to be the subset in which the AUPRC leveled off. Again, to confirm that results were consistent, the reduction process was repeated for each combination of re-sampling technique, resolution, and cell line. 
-->

### FINAL MODEL ASSESSMENT AND PERFORMANCE

Once the appropriate predictor type and set of features were determined, a final set of random forest classification models were performed to fully compare re-sampling techniques. To reduce bias due to random dataset generation, 10-fold cross validation was used <!--to tune the number of features to consider at each node and the number of ensemble trees to aggregate-->.The default number of features to consider at each node of the random forest algorithm was set as the square root of the number of features in the model. Likewise, the number of ensemble trees to aggregate was set at 500. All random forest algorithms were implemented in R using the `caret` package version 6.0 [@kuhn2012caret]. Once the model was implemented, it was assessed using the test set, and performance metrics were then recorded. 

The efficacy of each re-sampling technique was compared using the F1-score, balanced accuracy, informedness, and  MCC. These metrics are defined as follows:

$$F1 - Score = \frac{2TP}{2TP + FP + FN}$$

$$Balanced \quad Accuracy = \dfrac{1}{2}\left( \dfrac{TP}{TP+FN} + \dfrac{TN}{TN+FP} \right)$$

$$Informedness = \dfrac{TP}{TP+FN} + \dfrac{TN}{TN+FP} - 1$$

$$MCC = \frac{TP \times TN - FP \times FN}{\sqrt{ (TP + FP)(TP+FN)(TN+FP)(TN+FN) }}$$


Here, TP refers to the number of bins correctly identified as containing a TAD boundary (true positives), FP refers to the number of bins incorrectly identified as containing a TAD boundary (false positives), TN refers to the number of bins correctly identified as not containing a TAD boundary (true negatives), and FN refers to the number of bins incorrectly identified as not containing a TAD boundary. Each of these quantities are obtained from the confusion matrix created by validating the model on the testing set.

Lastly, for the task of identifying the genomic annotations that were most predictive of the formation of TAD boundaries, we evaluated the variable importances associated with each feature of the random forest model associated with the best combination of reduced predictor type and re-sampling technique, across each cell line. To calculate variable importances for each model, the AUPRC on the out-of-bag portion of the data within each tree was recorded. Then the same was done after permuting each predictor variable. The difference between the two AUPRC values was then averaged over all trees, and normalized by the standard error of the differences. All measures of importances were then rescaled to have a maximum value of 100 for better interpretation.

# RESULTS

## ENSEMBLE COMPARISONS

We evaluated and compared multiple random forest classification algorithms built on domain data extracted from Hi-C datasets for 2 cell lines C = {GM12878, K562}, at 4 different resolutions R = {10 kb, 25 kb, 50 kb, 100 kb}. For each classifier, we compared 3 predictor types P = {OC, OP, Distance} and 5 class balancing techniques B = {None, RUS, ROS, SMOTE, ROSE}. A system produced from the ensemble framework was defined as: E = {c, r, p, b}, where $c \in C, \quad R \in R, \quad p \in P,$ and $b \in$ B. Letting |X| denote the cardinality of each set, we evaluated $|C| \times |R| \times |P| \times |B|$ different ensemble systems, totaling X (=$x \times 4 \times 3 \times 5$) separate models. For each model, we aimed to assess both model performance and identify the genomic annotations that were most important in correctly labeling the genomic bins that overlapped with flanked TAD boundaries. 

## IMBALANCE RATIOS REMAINED SEVERE AND CONSISTENT ACROSS RESOLUTIONS

We first examined the class imbalance of the different data resolutions for each cell line (Supplementary Table 5). While the total number of genomic bins decreased as resolution increased, the imbalance ratios remained relatively stable. We found that there was an average imbalance ratio of 0.154 (SD=0.022) and 0.137 (SD=0.0181) across resolutions for the GM12878 and K562 cell lines respectively. The low imbalance ratios indicate a wide disparity between majority and minority classes, and therefore suitable resampling techniques were needed.  

## MANY GENOMIC ANNOTATIONS WERE HIGHLY CORRELATED

When assessing the collinearity of the genomic annotations, it was found that the total number of bases that the number overlapped bases between genomic bins flanked by a TAD boundary were highly correlated between many genomic annotations (). That is, many genomic annotations tended to colocalize near eachother at the boarders of domains. This was present across both resolution and cell line. Specifically, we found that the CTCF, Rad21, Smc3ab, and Znf143 transcription factor binding sites were highly colocalized near TAD boundaries. Likewise, many histone modifications also tended to appear near TAD boundaries in conjunction with one another including H3K4me1, H3K4me2, H3K4me3, H3K9ac, and H3K27ac. Furthermore, particular chromatin segmentation states, inclduing heterochromatin, repressed chromatin, poised promotors, and repetitive copy number variations, were found to be highly correlated with on another, yet inversely correlated with many transcription factor binding sites. The high correlations among many of the genomic annotations gives precedent for the use of an elastic-net regularization.




<!--
## DISTANCE TYPE PREDICTORS OUTPERFORM OTHER PREDICTOR-TYPES

For each cell line and resolution, we balanced the data using each resampling technique and performed a series of random forest classification algorithms on the full features space (all combined predictor types), as well as on type-specific feature spaces (overlap counts, overlap predictors, and distances). PR-curves were plotted and compared for 10 kb resolutions (Figures 4-8). For all curves, the baseline threshold used to assign a performance to a random model is defined as the ratio of genomic bins that overlapped with a flanked TAD boundary divided by the total number of genomic bins for the test set, respective to cell line and resolution. It was found that for the 10 kb resolution data, models with distance type predictors had consistently higher AUPRC values compared to other predictor types, across both cell line and resampling technique. This result was observed for the other resolutions as well (Supplementary Figuers 9-23l; Supplementary Table 2). 

Recursive feature elimination was used to establish the optimal number of distance-type features to include in downstream analyses. It was determined that, for 10 kb resolution data, near optimal performance was obtained for models with only 32 predictors, across each resampling technique and for each cell line (Figures 9-13). Results remained consistent across the other data resolutions (Supplementary Figures 24-38). Therefore, the feature space was reduced for subsequent models to the top 32 predictors, respective to cell line, resolution, and resampling technique. 
-->

## DATA RESAMPLING IMPROVES MODEL PREDICTION

 



# Discussion


# Abbreviations


# Acknowledgements

_Conflict of Interest._ None.

# Funding

# References
