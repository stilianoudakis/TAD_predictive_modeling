---
title: "Methods"
author: "Kellen Cresswell"
date: "October 14, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Methods

## Adjusted Precision

In the contact matrix domains, TAD boundaries are often noisy with imprecise locations. This noise can cause boundaries to shift by multiple bins depending on the TAD caller used. Additionally, we find that genomic features such as CTCF and RAD21 exist at elevated levels in bins around called TAD boundaries and not just in the bin containing the boundary itself. To account for this imprecision we use a flanked form of the precision metric called adjusted precision ($P_a$). In the framework of boundary prediction, traditional precision takes two inputs: true positives ($TP$) whch are correctly identified TAD boundaries and false positives ($FP$) which are regions incorrectly identified as TAD boundaries. Adjusted precision introduces a flanking region ($f$) such that true positives become all predicted TAD boundaries within $f$ of a true TAD boundary and false positives become those that are not within $f$ of a true TAD boundary. We calculate adjusted precision using the following formula:

$$
P_{a} = \frac{TP_{a}}{FP_{a}+TP_{a}}
$$

where $TP_{a}$ is the flanked true positive value and $FP_{a}$ is the flanked false positive value.

Conveniently we can convert the adjusted precision score to another measure "adjusted false discovery rate ($FDR_a$)" by simply taking 1-$P_{a}$. $FDR_a$ can be interpreted as the probability that a TAD boundary identified by a given approach is within $f$ of a true TAD boundary. 

## Choosing the neural network hyperparameters

A deep feed forward neural network was fit to predict the location of TAD boundaries. The ideal number of hidden layers (1,2) and nodes (4,8,12,16,32,64), and the resampling scheme ("none", "SMOTE", "RUS", "ROS") was tested based on cross-validation using adjusted precision as our measure of accuracy. To prevent overfitting, a dropout layer was added between every layer. In general, we find that networks with more than 1 layer tend to either overfit the data, resulting in a decrease in prediction quality. As a result, we present results for single layer models exclusively. In addition, we find the choice of activation function to be largely inconsequential to our predictive ability as a result a simple ReLU activation function is used for all layers.

# Results

## SMOTE sampling increases predictive ability compared to other methods

We find that SMOTE oversampling outperforms other oversampling methods in terms of TAD boundary detection (Figure 1). In general, we find that the all over sampling methods produce comparable results. However, we find that any oversampling method results in better precision than no oversampling at all. Without oversampling or undersampling, the vast majority of models (74%) fail to predict a single TAD boundary across all data types and resolutions. SMOTE has the smallest proportion of failures (0.5%) followed by RUS (0.9%) and ROS (2%). This demonstrates the need to balance the data before predicting TAD boundaries.

## TAD prediction is largely invariant to the number of layers

We find that regardless of the number of hidden layers (4,8,12,16,32,64), the neural network is able to predict TAD boundaries near those annotated by TAD callers. The number of layers with the best performance is 16 (Median $PPV_a$ = .941) and the worst is 4 (Median $PPV_a$ = .932). In rare circumstances (Figure 2a,2b,2c), we find that networks with too few layers will have poor predictive ability. As a result, we use a single layer network with 16 layers as our default architecture for comparison of features.

## Neural network approaches outperform logistic regression for complex datasets

To test, whether the neural network approaches can outperform simple models, we fit a logistic regression with variable selection using an elastic net algorithm and compare results. We find the for overlap, distance and percent overlap, both approaches perform similarly well (Figure 3). For the more complex combined dataset, the neural network approach produces slightly better results than the logistic regression approach. These results suggest a neural network may be optimal for more complex datasets and better at integrating future data.

## Histone modifications, broadHMM and transcription factor binding sites have similar predictive abilities

We fnd that the three tested categories produce similar results in terms of predictive ability. Transcription factor binding sites (Median $PPV_a$ = .941) produced the best results, followed by BroadHMM (Median $PPV_a$ = .940) and histone modifications (Median $PPV_a$ = .925). Figure 4 summarises these results, demonstrating the similarity. These results suggest that boundary prediction is robust to the type of data source used for prediction. 

## Distance provides the best prediction results

To determine the best feature for prediction, we compared distance, overlap, percent overlap and combination of all features. We find that simply using distance (Median $PPV_a$ = .950) outperforms overlap (Median $PPV_a$ = .930) and percent overlap (Median $PPV_a$ = .915). Additionally, we find that the combined (Median $PPV_a$ = .940) approach outperforms overlap and percent overlap but not distance. 

## Predicted TADs tend to be near true TADs

To determine the accuracy of predicted TADs in terms of raw distance, we calculate the mean distance of predicted TADs and non-predicted TADs from annotated ones. We find a large difference in mean distance (64.2 vs. 5.87) between predicted TADs and non-predicted TADs. These results suggest that our method is able to succesfully differentiate random regions and TAD regions.

## The profile of TAD predictors is unique to the data type

Distance from predicted TAD boundaries was calculated for a range of features, seperated by resolution. Features were clustered using Ward.D2 clustering and visualized using a heatmap (Figure 5). TFBS and BroadHMM can be seperated into three distinct groups (Enriched, Depleted and unrelated). The core enriched TFBS group includes CTCF, RAD21, Pu1 and Smc3ab9263. For BroadHMM, the core group is heterochromatin, insulators and weak enhancers. In TFBS, there is heavy depletion in Nfe2 and Tr4. In BroadHMM, repetive CNVs, poised promoters and Txn transitions are all depleted. In the case of histones, a different pattern emerges with two distinct groups (a large enriched group and a smaller depleted group). The large group consists of H3k27me3, H3k9me3, H2az, H3k4me1, H3k4me2, H327ac and H3k4me3. The depleted group consists of H3k36me3, H3k79me2, H3k9ac, H4k20me1. 

Summarized across all resolutions and features, we find that H3K9me3 co-locates the closest with predicted TAD boundaries followed by heterochromatin. Of the top 10 features, 6 are histones, 2 are from BroadHMM (heterochromatin, insulators) and 2 are TFBS (CTCF, Pu1).



Boxplot with distance from predicted and TAD caller predicted
Plot from Spiros paper with predicted and sites around boundaries


